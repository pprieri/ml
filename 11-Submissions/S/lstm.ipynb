{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 2s 2ms/step - loss: 0.6560\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5022\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.3277\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0616\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -0.2946\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -0.7623\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -1.2193\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -1.6469\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -1.8966\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -2.1373\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -2.3060\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -2.4359\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: -2.5286\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: -2.6502\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -2.7320\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: -2.8394\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -2.9113\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.0114\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.0905\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: -3.1727\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.2554\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.3232\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -3.4152\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -3.4995\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -3.5778\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -3.6545\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.7256\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.8027\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 14ms/step - loss: -3.8764\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -3.9510\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.0317\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.0942\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -4.1714\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.2474\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.3085\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -4.3819\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.4660\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -4.5207\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.5980\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.6710\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -4.7448\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -4.8164\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -4.8800\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -4.9470\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.0201\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.0913\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -5.1556\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.2227\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.2928\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -5.3704\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.4244\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.5020\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.5582\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -5.6372\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -5.7015\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: -5.7775\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -5.8261\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -5.9062\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -5.9797\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.0417\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.1043\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.1822\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.2392\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.3177\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -6.3691\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.4439\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.5158\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -6.5606\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -6.6493\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.7081\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -6.7695\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.8426\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.9141\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -6.9830\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -7.0416\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.0992\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -7.1775\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.2445\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.3077\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -7.3798\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.4401\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -7.4980\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.5758\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -7.6423\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.7078\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -7.7648\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.8329\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -7.9059\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: -7.9661\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -8.0371\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -8.1020\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: -8.1574\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -8.2281\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -8.2923\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -8.3684\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -8.4325\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -8.4946\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -8.5591\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: -8.6223\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: -8.6878\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Sequence:\n",
      "[0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
      "\n",
      "Predictions:\n",
      "[3.3200316e-22, 3.3209689e-22, 3.3127193e-22, 3.2572824e-22, 3.1546224e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22, 3.3211082e-22]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the input sequence\n",
    "# sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "\n",
    "# Create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(sequence) - 6):\n",
    "    X_train.append(sequence[i:i+5])\n",
    "    y_train.append(sequence[i+5])\n",
    "\n",
    "# Convert the training data to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the input data to match LSTM input shape (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Create and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(5, 1)))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# Generate predictions for the next 20 periods\n",
    "prediction_sequence = sequence.copy()\n",
    "for _ in range(20):\n",
    "    last_sequence = np.array(prediction_sequence[-5:]).reshape(1, 5, 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    prediction_sequence.append(prediction[0, 0])\n",
    "\n",
    "# Print the sequence and predictions\n",
    "print(\"Sequence:\")\n",
    "print(sequence)\n",
    "print(\"\\nPredictions:\")\n",
    "print(prediction_sequence[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 2ms/step - loss: 0.1830\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1745\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1645\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1570\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1554\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1486\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1455\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1417\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1386\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1370\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1340\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1315\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1301\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1267\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1247\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1223\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1216\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1186\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1162\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1140\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1135\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1100\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1118\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1033\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1026\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0978\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0972\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0960\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0928\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0929\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0897\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0886\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0896\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0806\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0788\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0781\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0767\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0748\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0747\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0747\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0730\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0713\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0707\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0692\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0693\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0693\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0678\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0674\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0668\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0667\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0671\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0666\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0649\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0663\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0666\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0658\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0639\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0634\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0627\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0632\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0630\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0635\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0634\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0647\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0669\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0616\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0623\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0604\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0613\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0616\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0611\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0616\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0615\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0598\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0610\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0608\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0590\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0602\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0591\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0622\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0592\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Sequence:\n",
      "[-1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
      "\n",
      "Predictions:\n",
      "[-0.035830878, -0.0017374493, 0.061910536, 0.0057060868, -0.3587797, -0.6149871, -0.1560514, 0.10992862, 0.13413005, -0.13598488, -0.44485146, -0.5247183, -0.11694637, 0.1804532, 0.10957439, -0.25522146, -0.48003843, -0.46021456, -0.030454982, 0.22521707]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Define the input sequence\n",
    "sequence = [-1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "# Create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(sequence) - 6):\n",
    "    X_train.append(sequence[i:i+5])\n",
    "    y_train.append(sequence[i+5])\n",
    "\n",
    "# Convert the training data to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the input data to match RNN input shape (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Create and train the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(6, input_shape=(5,1)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# Generate predictions for the next 20 periods\n",
    "prediction_sequence = sequence.copy()\n",
    "for _ in range(20):\n",
    "    last_sequence = np.array(prediction_sequence[-5:]).reshape(1, 5, 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    prediction_sequence.append(prediction[0, 0])\n",
    "\n",
    "# Print the sequence and predictions\n",
    "print(\"Sequence:\")\n",
    "print(sequence)\n",
    "print(\"\\nPredictions:\")\n",
    "print(prediction_sequence[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 2ms/step - loss: 0.2132\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1324\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1269\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1196\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1146\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1119\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1015\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0982\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0979\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0971\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0995\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0951\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0925\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0932\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0921\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0935\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0912\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0946\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1232\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1092\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1016\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0958\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0955\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0927\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0871\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0907\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0949\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0914\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0906\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0913\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0875\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0877\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0903\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0890\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0876\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0874\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0856\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0885\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0868\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0925\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0862\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0889\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0903\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1109\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.1044\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.1092\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1221\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1202\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1077\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0912\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0884\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0850\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0848\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0846\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0842\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0841\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0835\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0832\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0848\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0845\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0841\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0854\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0840\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0844\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0824\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0826\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0851\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0890\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0948\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Sequence:\n",
      "[0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
      "\n",
      "Predictions:\n",
      "[-0.25467882, -0.062823355, 0.015089072, 0.006221324, -0.22068903, -0.26397675, -0.2199238, -0.15012299, -0.14697662, -0.1389063, -0.16917711, -0.18454927, -0.17874557, -0.16771364, -0.16501729, -0.1634828, -0.16696808, -0.17008612, -0.1699821, -0.1683132]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Define the input sequence\n",
    "sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "# Create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(sequence) - 6):\n",
    "    X_train.append(sequence[i:i+5])\n",
    "    y_train.append(sequence[i+5])\n",
    "\n",
    "# Convert the training data to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the input data to match Conv1D input shape (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Create and train the Conv1D model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, 3, activation='relu', input_shape=(5, 1)))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# Generate predictions for the next 20 periods\n",
    "prediction_sequence = sequence.copy()\n",
    "for _ in range(20):\n",
    "    last_sequence = np.array(prediction_sequence[-5:]).reshape(1, 5, 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    prediction_sequence.append(prediction[0, 0])\n",
    "\n",
    "# Print the sequence and predictions\n",
    "print(\"Sequence:\")\n",
    "print(sequence)\n",
    "print(\"\\nPredictions:\")\n",
    "print(prediction_sequence[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "[-0.1917727, -0.063089885, -0.15800852, -0.049254768, -0.17138362, -0.15517037, -0.15996483, -0.11973558, -0.1231083, -0.1091617, -0.11546079, -0.12441074, -0.1264229, -0.1235144, -0.12220915, -0.12116367, -0.11957668, -0.12073357, -0.12133762, -0.121839635]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions:\")\n",
    "print(prediction_sequence[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 2ms/step - loss: 0.1746 \n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1569\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1475\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1490\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1458\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1513\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1422\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1395\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1409\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1423\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1342\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1379\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1352\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1325\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1307\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1296\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1283\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1303\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1285\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1242\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1214\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1216\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1189\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1160\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1174\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1151\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1167\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1200\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1135\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.1111\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1107\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1015\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1030\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1030\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0973\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0976\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0952\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0966\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0979\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0934\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0850\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0827\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0837\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0765\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0773\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0740\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0752\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0720\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0703\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0689\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0675\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0692\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0670\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0693\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0713\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0629\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0635\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0596\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0586\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0580\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0643\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0603\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0569\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0574\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0561\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0595\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0537\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0565\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0522\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0524\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0557\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0505\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0508\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0519\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0538\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0522\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0557\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0570\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0524\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0490\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0496\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0494\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0494\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0478\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026FE1F570D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Sequence:\n",
      "[0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
      "\n",
      "Predictions:\n",
      "[-0.012723889, 0.076580465, -0.036123708, -0.14797778, -0.20777267, -0.5380621, -0.051169634, 0.016768977, -0.087582335, -0.18327764, -0.23153213, -0.33398306, -0.08545977, -0.04976634, -0.12146879, -0.18930224, -0.21852419, -0.23139364, -0.11267547, -0.09677534]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the input sequence\n",
    "sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "# Create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(sequence) - 6):\n",
    "    X_train.append(sequence[i:i+5])\n",
    "    y_train.append(sequence[i+5])\n",
    "\n",
    "# Convert the training data to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the input data to match LSTM input shape (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Create and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(5, 1)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# Generate predictions for the next 20 periods\n",
    "prediction_sequence = sequence.copy()\n",
    "for _ in range(20):\n",
    "    last_sequence = np.array(prediction_sequence[-5:]).reshape(1, 5, 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    prediction_sequence.append(prediction[0, 0])\n",
    "\n",
    "# Print the sequence and predictions\n",
    "print(\"Sequence:\")\n",
    "print(sequence)\n",
    "print(\"\\nPredictions:\")\n",
    "print(prediction_sequence[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the sequence\n",
    "sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "# Create lagged features DataFrame\n",
    "lagged_df = pd.DataFrame({'Sequence': sequence})\n",
    "for lag in range(1, 13):\n",
    "    lagged_df[f'Lag_{lag}'] = lagged_df['Sequence'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "lagged_df.dropna(inplace=True)\n",
    "\n",
    "# Prepare the data\n",
    "X = lagged_df.drop('Sequence', axis=1)\n",
    "y = lagged_df['Sequence']\n",
    "\n",
    "# Create and train the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Predict the next 12 values in the sequence\n",
    "next_sequence = sequence[-12:]\n",
    "next_X = pd.DataFrame({'Lag_{}'.format(lag): [next_sequence[-lag]] for lag in range(1, 13)})\n",
    "predicted_values = rf_classifier.predict(next_X)\n",
    "\n",
    "# Print the predicted values\n",
    "print(\"Predicted values:\", predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag_1</th>\n",
       "      <th>Lag_2</th>\n",
       "      <th>Lag_3</th>\n",
       "      <th>Lag_4</th>\n",
       "      <th>Lag_5</th>\n",
       "      <th>Lag_6</th>\n",
       "      <th>Lag_7</th>\n",
       "      <th>Lag_8</th>\n",
       "      <th>Lag_9</th>\n",
       "      <th>Lag_10</th>\n",
       "      <th>Lag_11</th>\n",
       "      <th>Lag_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lag_1  Lag_2  Lag_3  Lag_4  Lag_5  Lag_6  Lag_7  Lag_8  Lag_9  Lag_10  \\\n",
       "0     -1      0      0      0      0      0     -1      0      0       0   \n",
       "\n",
       "   Lag_11  Lag_12  \n",
       "0       0       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "Step 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "# Define the sequence\n",
    "sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "# Create lagged features DataFrame\n",
    "lagged_df = pd.DataFrame({'Sequence': sequence})\n",
    "for lag in range(1, 13):\n",
    "    lagged_df[f'Lag_{lag}'] = lagged_df['Sequence'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "lagged_df.dropna(inplace=True)\n",
    "\n",
    "# Prepare the data\n",
    "X = lagged_df.drop('Sequence', axis=1)\n",
    "y = lagged_df[['Sequence']]\n",
    "\n",
    "# Create and train the ClassifierChain with RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "classifier_chain = ClassifierChain(base_estimator=rf_classifier)\n",
    "classifier_chain.fit(X, y)\n",
    "\n",
    "# Predict the next 12 values in the sequence\n",
    "next_sequence = sequence[-12:]\n",
    "next_X = pd.DataFrame({'Lag_{}'.format(lag): [next_sequence[-lag]] for lag in range(1, 13)})\n",
    "predicted_values = classifier_chain.predict(next_X)\n",
    "\n",
    "# Print the predicted values\n",
    "print(\"Predicted values:\")\n",
    "for i, value in enumerate(predicted_values[0]):\n",
    "    print(f\"Step {i+1}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skforecast\n",
      "  Downloading skforecast-0.9.0-py2.py3-none-any.whl (365 kB)\n",
      "     -------------------------------------- 365.3/365.3 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib<1.4,>=1.1.0 in c:\\python\\python39\\lib\\site-packages (from skforecast) (1.1.0)\n",
      "Collecting optuna<3.3,>=2.10.0\n",
      "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
      "     -------------------------------------- 390.6/390.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<4.66,>=4.57.0 in c:\\python\\python39\\lib\\site-packages (from skforecast) (4.62.3)\n",
      "Requirement already satisfied: scikit-learn<1.4,>=1.0 in c:\\python\\python39\\lib\\site-packages (from skforecast) (1.1.2)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in c:\\python\\python39\\lib\\site-packages (from skforecast) (1.21.3)\n",
      "Requirement already satisfied: pandas<2.1,>=1.2 in c:\\python\\python39\\lib\\site-packages (from skforecast) (2.0.2)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
      "     ------------------------------------ 224.5/224.5 kB 915.8 kB/s eta 0:00:00\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "     ------------------------------------ 151.6/151.6 kB 210.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python\\python39\\lib\\site-packages (from optuna<3.3,>=2.10.0->skforecast) (21.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\python\\python39\\lib\\site-packages (from optuna<3.3,>=2.10.0->skforecast) (1.4.29)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python\\python39\\lib\\site-packages (from pandas<2.1,>=1.2->skforecast) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\python39\\lib\\site-packages (from pandas<2.1,>=1.2->skforecast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python39\\lib\\site-packages (from pandas<2.1,>=1.2->skforecast) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python39\\lib\\site-packages (from scikit-learn<1.4,>=1.0->skforecast) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\python\\python39\\lib\\site-packages (from scikit-learn<1.4,>=1.0->skforecast) (1.7.2)\n",
      "Requirement already satisfied: colorama in c:\\python\\python39\\lib\\site-packages (from tqdm<4.66,>=4.57.0->skforecast) (0.4.4)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     -------------------------------------- 78.7/78.7 kB 547.5 kB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\python39\\lib\\site-packages (from packaging>=20.0->optuna<3.3,>=2.10.0->skforecast) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.1,>=1.2->skforecast) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\python39\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna<3.3,>=2.10.0->skforecast) (1.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\python\\python39\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (2.0.1)\n",
      "Installing collected packages: typing-extensions, PyYAML, Mako, colorlog, cmaes, alembic, optuna, skforecast\n",
      "Successfully installed Mako-1.2.4 PyYAML-6.0 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0 skforecast-0.9.0 typing-extensions-4.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install skforecast --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`y` must be a pandas Series.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10000/3101959340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mlags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mforecaster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Predict the next 12 values in the sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y, exog, store_in_sample_residuals)\u001b[0m\n\u001b[0;32m    478\u001b[0m                  \u001b[0mexog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_train_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_sample_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py\u001b[0m in \u001b[0;36mcreate_train_X_y\u001b[1;34m(self, y, exog)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mcheck_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         y = transform_series(\n\u001b[0;32m    324\u001b[0m                 \u001b[0mseries\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\skforecast\\utils\\utils.py\u001b[0m in \u001b[0;36mcheck_y\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`y` must be a pandas Series.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: `y` must be a pandas Series."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "# from skforecast import metrics\n",
    "\n",
    "# Define the sequence\n",
    "sequence = [0, 0, -10, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1]\n",
    "\n",
    "# Create lagged features DataFrame\n",
    "lagged_df = pd.DataFrame({'Sequence': sequence})\n",
    "for lag in range(1, 13):\n",
    "    lagged_df[f'Lag_{lag}'] = lagged_df['Sequence'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "lagged_df.dropna(inplace=True)\n",
    "\n",
    "# Prepare the data\n",
    "X = lagged_df.drop('Sequence', axis=1)\n",
    "y = lagged_df['Sequence']\n",
    "\n",
    "# Create and train the ForecasterAutoreg model with AutoARIMA\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=RandomForestClassifier(),\n",
    "    lags=range(1, 13)\n",
    ")\n",
    "forecaster.fit(X, y)\n",
    "\n",
    "# Predict the next 12 values in the sequence\n",
    "next_sequence = sequence[-12:]\n",
    "next_X = pd.DataFrame({'Lag_{}'.format(lag): [next_sequence[-lag]] for lag in range(1, 13)})\n",
    "predicted_values = forecaster.predict(steps_ahead=12, future_X=next_X)\n",
    "\n",
    "# Print the predicted values\n",
    "print(\"Predicted values:\", predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
